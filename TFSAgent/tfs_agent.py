import os
import time
import uuid
import json
import datetime
import logging
import argparse

from llm_api import query_llm
from utils import (
    download_paper,
    pdf_to_text_with_structure,
    insert_to_notion,
    parse_json_blob
)

# é…ç½®æ—¥å¿—è®°å½•
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)

# ä» prompt æ–‡ä»¶ä¸­è¯»å–ç³»ç»Ÿæç¤º
sys_prompt_file = "TFSAgent/prompt.txt"
if not os.path.exists(sys_prompt_file):
    logger.error("æœªæ‰¾åˆ°ç³»ç»Ÿæç¤ºæ–‡ä»¶: %s", sys_prompt_file)
    exit(1)
with open(sys_prompt_file, "r", encoding="utf-8") as f:
    sys_prompt = f.read().strip()


def call_llm(sys_prompt: str, user_prompt: str) -> str:
    response = query_llm(sys_prompt, user_prompt)
    return response


def parse_args():
    parser = argparse.ArgumentParser(
        description="Agentï¼šä» arXiv ä¸‹è½½è®ºæ–‡ï¼Œè°ƒç”¨ LLM è§£æï¼Œå¹¶ä¸Šä¼ è‡³ Notionã€‚"
    )
    parser.add_argument(
        "--max_results",
        type=int,
        default=25,
        help="ä¸‹è½½çš„è®ºæ–‡æœ€å¤§æ•°é‡ï¼Œé»˜è®¤ä¸º 30"
    )
    parser.add_argument(
        "--delete",
        action="store_true",
        default=True,
        help="å¦‚æœè®¾ç½®ï¼Œåˆ™å¤„ç†å®Œååˆ é™¤ä¸‹è½½çš„ PDF æ–‡ä»¶"
    )
    parser.add_argument(
        "--wait_time",
        type=int,
        default=20,
        help="ä¸‹è½½åç­‰å¾…çš„ç§’æ•°ï¼ˆé»˜è®¤ 20 ç§’ï¼‰"
    )
    return parser.parse_args()


def main():
    args = parse_args()

    logger.info("[TFSğŸ˜‹]: Downloading papers from arXiv (max_results=%d)...", args.max_results)
    download_paper(max_results=args.max_results)
    logger.info("[TFSğŸ˜‹]: Papers downloading, waiting for %d seconds.", args.wait_time)
    time.sleep(args.wait_time)

    download_dir = r'TFSAgent\paper'
    pdf_files = [f for f in os.listdir(download_dir) if f.lower().endswith(".pdf")]
    if not pdf_files:
        logger.info("[TFSğŸ˜¨]: No PDF files found in the download directory.")
        return

    for pdf_file in pdf_files:
        pdf_path = os.path.join(download_dir, pdf_file)
        logger.info("[TFSğŸ˜‹]: Processing PDF file: %s", pdf_file)

        # æå– PDF æ–‡æœ¬ï¼ˆä¿ç•™é¡µé¢ç»“æ„ï¼‰
        extracted_lines = pdf_to_text_with_structure(pdf_path)
        user_prompt = "\n".join(extracted_lines)

        # è°ƒç”¨ llm æ¥å£ï¼Œä¼ å…¥ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æç¤º
        llm_response = call_llm(sys_prompt, user_prompt)

        try:
            result_data = parse_json_blob(llm_response)
        except Exception as e:
            logger.error("[TFSğŸ˜¨]: Error parsing LLM response for %s: %s", pdf_file, e)
            continue

        # ç³»ç»Ÿè‡ªåŠ¨è®¾ç½®ä¸Šä¼ æ—¶é—´ï¼Œå¹¶ç”Ÿæˆå”¯ä¸€ id
        result_data["upload_time"] = datetime.datetime.now().isoformat()
        # ä½¿ç”¨ä¸¤ä¸ªä¸åŒå­—æ®µï¼Œåˆ†åˆ«å¯ä½œä¸º Notion é¡µé¢å”¯ä¸€æ ‡è¯†æˆ–å…¶ä»–ç”¨é€”
        result_data["id"] = str(uuid.uuid4())
        result_data["Id"] = str(uuid.uuid4())

        # å¯å°†æº PDF æ–‡ä»¶ååŠ å…¥æ•°æ®åŒ…ï¼Œæ–¹ä¾¿è¿½è¸ª
        result_data["source_pdf"] = pdf_file

        # ä¸Šä¼ æ•°æ®è‡³ Notion
        insert_to_notion(result_data)
        logger.info("[TFSğŸ˜‹]: Uploaded %s to Notion.", pdf_file)

    if args.delete:
        logger.info("[TFSğŸ˜‹]: Finished processing, deleting downloaded PDF files...")
        for pdf_file in pdf_files:
            pdf_path = os.path.join(download_dir, pdf_file)
            os.remove(pdf_path)
        logger.info("[TFSğŸ˜‹]: PDF files deleted, agent task completed.")
    else:
        logger.info("[TFSğŸ˜‹]: Agent task completed, PDF files retained.")


if __name__ == "__main__":
    main()
