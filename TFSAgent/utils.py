import json
import os
from typing import Any, Dict
import re
import arxiv
from dotenv import load_dotenv
import logging
import os
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
load_dotenv()

def download_paper(max_results=25):
    """
    ä¸‹è½½ arXiv ä¸Šæœ€æ–°çš„ LLM ç›¸å…³è®ºæ–‡ã€‚
    åŸºäºarxivå®˜æ–¹apiå®ç°
    ç›®å‰arxivä¸æ”¯æŒæ ¹æ®æ—¥æœŸç²¾å‡†æœç´¢ï¼Œä¸è¿‡æ®æˆ‘è§‚å¯Ÿllm based agenté¢†åŸŸä¸€å¤©åå‡ ç¯‡ï¼Œå› æ­¤æ¯æ¬¡ä¸‹è½½25ç¯‡å¹¶è·³è¿‡ä¹‹å‰é‡å¤è¿‡çš„
    
    å‚æ•°:
      max_results (int): ä¸‹è½½çš„è®ºæ–‡æœ€å¤§æ•°é‡ï¼Œé»˜è®¤ä¸º 25ã€‚
    """
    download_dir = r'TFSAgent\\paper'
    ids_dir = r'TFSAgent\\ids'
    # ç¡®ä¿ ids ç›®å½•å­˜åœ¨
    if not os.path.exists(ids_dir):
        os.makedirs(ids_dir)
    ids_file = os.path.join(ids_dir, 'downloaded_ids.txt')


    if not os.path.exists(download_dir):
        os.makedirs(download_dir)

    
    downloaded_ids = set()
    if os.path.exists(ids_file):
        with open(ids_file, 'r', encoding='utf-8') as f:
            for line in f:
                downloaded_ids.add(line.strip())

    client = arxiv.Client()
    search = arxiv.Search(
        query="all:llm AND all:agent",
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )

    for r in client.results(search):

        paper_id = r.get_short_id() if hasattr(r, "get_short_id") else r.entry_id.split('/')[-1]
        if paper_id not in downloaded_ids:
            
            logger.debug(f"[TFSğŸ˜‹]Downloading new paper: {paper_id}")
            r.download_pdf(dirpath=download_dir)
            downloaded_ids.add(paper_id)
        else:
           
            logger.debug(f"[TFSğŸ˜‹]Skipped {paper_id}, already processsed.")


    with open(ids_file, 'w', encoding='utf-8') as f:
        for pid in downloaded_ids:
            f.write(pid + "\n")


def make_json_serializable(obj: Any) -> Any:
    """Recursive function to make objects JSON serializable"""
    if obj is None:
        return None
    elif isinstance(obj, (str, int, float, bool)):
        # Try to parse string as JSON if it looks like a JSON object/array
        if isinstance(obj, str):
            try:
                if (obj.startswith("{") and obj.endswith("}")) or (obj.startswith("[") and obj.endswith("]")):
                    parsed = json.loads(obj)
                    return make_json_serializable(parsed)
            except json.JSONDecodeError:
                pass
        return obj
    elif isinstance(obj, (list, tuple)):
        return [make_json_serializable(item) for item in obj]
    elif isinstance(obj, dict):
        return {str(k): make_json_serializable(v) for k, v in obj.items()}
    elif hasattr(obj, "__dict__"):
        # For custom objects, convert their __dict__ to a serializable format
        return {"_type": obj.__class__.__name__, **{k: make_json_serializable(v) for k, v in obj.__dict__.items()}}
    else:
        # For any other type, convert to string
        return str(obj)


def parse_json_blob(json_blob: str) -> Dict[str, str]:
    try:
        first_accolade_index = json_blob.find("{")
        last_accolade_index = [a.start() for a in list(re.finditer("}", json_blob))][-1]
        json_blob = json_blob[first_accolade_index : last_accolade_index + 1].replace('\\"', "'")
        json_data = json.loads(json_blob, strict=False)
        return json_data
    except json.JSONDecodeError as e:
        place = e.pos
        if json_blob[place - 1 : place + 2] == "},\n":
            raise ValueError(
                "JSON is invalid: you probably tried to provide multiple tool calls in one action. PROVIDE ONLY ONE TOOL CALL."
            )
        raise ValueError(
            f"The JSON blob you used is invalid due to the following error: {e}.\n"
            f"JSON blob was: {json_blob}, decoding failed on that specific part of the blob:\n"
            f"'{json_blob[place - 4 : place + 5]}'."
        )
    except Exception as e:
        raise ValueError(f"Error in parsing the JSON blob: {e}")

import pdfplumber
import json

import pdfplumber

def group_words_to_lines(words, threshold=3):
    """
    å°† pdfplumber æå–çš„ word åˆ—è¡¨æŒ‰è¡Œèšåˆï¼Œ
    threshold ä¸ºç›¸åŒè¡Œä¹‹é—´å…è®¸çš„ y åæ ‡å·®å¼‚ã€‚
    è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸€é¡¹ä¸ºä¸€è¡Œçš„æ–‡æœ¬åŠå…¶å¤§è‡´ y åæ ‡ã€‚
    """
    lines = []
    # å…ˆæŒ‰ç…§ top åæ ‡æ’åº
    words = sorted(words, key=lambda w: w["top"])
    
    for word in words:
        placed = False
        for line in lines:
            # å¦‚æœ word ä¸è¯¥è¡Œçš„ top åæ ‡å·®è·å°äº thresholdï¼Œåˆ™å½’åˆ°åŒä¸€è¡Œ
            if abs(word["top"] - line["top"]) < threshold:
                line["words"].append(word)
                # æ›´æ–°è¡Œçš„ top ä¸ºå¹³å‡å€¼ï¼ˆå¯é€‰ï¼‰
                line["top"] = (line["top"] * (len(line["words"]) - 1) + word["top"]) / len(line["words"])
                placed = True
                break
        if not placed:
            # æ–°å»ºä¸€è¡Œ
            lines.append({"top": word["top"], "words": [word]})
    # å¯¹æ¯è¡Œå†…çš„å•è¯æŒ‰ x0 åæ ‡æ’åºï¼Œå¹¶ç”Ÿæˆæ–‡æœ¬
    result = []
    for line in lines:
        line_words = sorted(line["words"], key=lambda w: w["x0"])
        text_line = " ".join(w["text"] for w in line_words)
        result.append({"top": line["top"], "type": "text", "content": text_line})
    return result

def pdf_to_text_with_structure(pdf_path: str) -> None:
    """
    ä» PDF ä¸­æå–çº¯æ–‡æœ¬å’Œç»“æ„åŒ–è¡¨æ ¼æ•°æ®ï¼ŒæŒ‰ç…§åœ¨é¡µé¢ä¸­çš„å‚ç›´é¡ºåºåˆå¹¶ï¼Œ
    æœ€åè¾“å‡ºåˆ°ä¸€ä¸ª TXT æ–‡ä»¶ä¸­ã€‚
    
    è¡¨æ ¼ä¼šä»¥ [Table] æ ‡è®°è¾“å‡ºï¼Œæ¯è¡Œçš„å„å•å…ƒæ ¼ä¹‹é—´ä»¥åˆ¶è¡¨ç¬¦åˆ†éš”ã€‚
    """
    output_lines = []
    
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            # æ·»åŠ é¡µçœ‰ä¿¡æ¯
            output_lines.append(f"=== Page {page.page_number} ===")
            
            # æå–é¡µé¢æ‰€æœ‰å•è¯ï¼Œå¹¶èšåˆæˆè¡Œ
            words = page.extract_words()
            text_blocks = group_words_to_lines(words)
            
            # æŸ¥æ‰¾é¡µé¢ä¸­çš„è¡¨æ ¼ï¼Œpdfplumber çš„ find_tables() è¿”å› Table å¯¹è±¡åˆ—è¡¨ï¼Œ
            # æ¯ä¸ª Table å¯¹è±¡æœ‰ bbox å±æ€§å’Œ extract() æ–¹æ³•æå–äºŒç»´åˆ—è¡¨æ•°æ®
            table_blocks = []
            tables = page.find_tables()
            for table in tables:
                bbox = table.bbox  # (x0, top, x1, bottom)
                # ä½¿ç”¨è¡¨æ ¼æ‰€åœ¨åŒºåŸŸçš„ top ä½œä¸ºæ’åºä¾æ®
                table_data = table.extract()
                table_blocks.append({"top": bbox[1], "type": "table", "content": table_data})
            
            # åˆå¹¶æ–‡æœ¬å—å’Œè¡¨æ ¼å—ï¼Œå¹¶æŒ‰ top åæ ‡æ’åº
            blocks = text_blocks + table_blocks
            blocks = sorted(blocks, key=lambda b: b["top"])
            
            # éå†åˆå¹¶åçš„å—ï¼Œæ„é€ è¾“å‡º
            for block in blocks:
                if block["type"] == "text":
                    output_lines.append(block["content"])
                elif block["type"] == "table":
                    output_lines.append("[Table]:")
                    # å°†è¡¨æ ¼æ¯ä¸€è¡ŒæŒ‰åˆ¶è¡¨ç¬¦è¿æ¥
                    for row in block["content"]:
                        output_lines.append("\t".join(cell if cell is not None else "" for cell in row))
            output_lines.append("")  # ç©ºè¡Œåˆ†éš”é¡µé¢å†…å®¹
    return output_lines



import requests
import json
# notion_token = os.environ.get("NOTION_TOKEN")
# database_id = os.environ.get("NOTION_DATABASE_ID")
def insert_to_notion(data: dict) -> None:
    """
    å°†åŒ…å«è®ºæ–‡ä¿¡æ¯çš„ JSON æ•°æ®æ’å…¥ Notion æ•°æ®åº“ä¸­ã€‚
    
    å‚æ•°ï¼š
      data: åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸ï¼Œä¾‹å¦‚ï¼š
            {
                "upload_time": "2023-07-28T10:00:00+08:00",
                "paper_title": "æŸè®ºæ–‡æ ‡é¢˜",
                "abstract": "è®ºæ–‡çš„æ‘˜è¦å†…å®¹â€¦â€¦",
                "research_deficiencies": "å½“å‰ç ”ç©¶å­˜åœ¨çš„é—®é¢˜â€¦â€¦"
            }
      notion_token: ä½ çš„ Notion é›†æˆçš„ API Tokenï¼ˆBearer Tokenï¼‰ã€‚
      database_id: Notion æ•°æ®åº“çš„ IDã€‚
    """
    url = "https://api.notion.com/v1/pages"
    notion_token = os.environ.get("NOTION_TOKEN")
    database_id = os.environ.get("NOTION_DATABASE_ID")
    headers = {
        "Authorization": f"Bearer {notion_token}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }
    
    payload = {
        "parent": {"database_id": database_id},
        "properties": {
            "Id": {
                "title": [{ "type": "text", "text": { "content": "Tomatoes" } }]
            },
            "Upload Time": {
                "date": {"start": data.get("upload_time")}
            },

            "Paper Title": {
                "rich_text": [
                    {
                        "text": {"content": data.get("paper_title", "")}
                    }
                ]
            },
            "Abstract": {
                "rich_text": [
                    {
                        "text": {"content": data.get("abstract", "")}
                    }
                ]
            },
            "Research Deficiencies": {
                "rich_text": [
                    {
                        "text": {"content": data.get("research_deficiencies", "")}
                    }
                ]
            },
            "Methodology": {
                "rich_text": [
                    {
                        "text": {"content": data.get("Methodology", "")}
                    }
                ]
            },
            "Experiments": {
                "rich_text": [
                    {
                        "text": {"content": data.get("Experiments", "")}
                    }
                ]
            },
            "Conclusion": {
                "rich_text": [
                    {
                        "text": {"content": data.get("Conclusion", "")}
                    }
                ]
            }
            
        }
    }
    
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        logger.info(f"[TFSğŸ˜‹]: Notion database updated successfully")
    else:
        logger.error(f"[TFSğŸ˜¨]: Notion database updated failed: {response.status_code}")
        logger.error(f"[TFSğŸ˜¨]: err: {response.text}")




